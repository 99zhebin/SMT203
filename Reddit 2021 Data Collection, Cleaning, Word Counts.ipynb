{"nbformat":4,"nbformat_minor":5,"metadata":{"colab":{"name":"Reddit 2021 Data Collection, Cleaning, Word Counts.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xck92cwllJ9n"},"source":["Praw installation"],"id":"xck92cwllJ9n"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcb93a80","executionInfo":{"status":"ok","timestamp":1635520918813,"user_tz":-480,"elapsed":3285,"user":{"displayName":"LIM XING YU _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07708069129339771923"}},"outputId":"aea6c759-411a-473f-cbfb-f9ffb817148e"},"source":["!pip install praw\n","!pip install pandas"],"id":"fcb93a80","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting praw\n","  Downloading praw-7.4.0-py3-none-any.whl (167 kB)\n","\u001b[?25l\r\u001b[K     |██                              | 10 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 40 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 51 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 61 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 71 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 81 kB 12.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 92 kB 11.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 133 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 143 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 153 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 163 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 167 kB 12.6 MB/s \n","\u001b[?25hCollecting update-checker>=0.18\n","  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n","Collecting prawcore<3,>=2.1\n","  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n","Collecting websocket-client>=0.54.0\n","  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.5.30)\n","Installing collected packages: websocket-client, update-checker, prawcore, praw\n","Successfully installed praw-7.4.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.1\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":527},"id":"16226439","executionInfo":{"status":"error","timestamp":1635520929669,"user_tz":-480,"elapsed":10868,"user":{"displayName":"LIM XING YU _","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07708069129339771923"}},"outputId":"6a1e5b19-023a-4d76-fb62-6d10ab1af810"},"source":["# Add Google Drive as an accessible path (Optional if you are running from Jupyter Notebook)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# change path to the designated google drive folder\n","# otherwise, data will be saved in /content folder which you may have issue locating\n","%cd /content/drive/My Drive/Colab Notebooks/SMT203 Project"],"id":"16226439","execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-0526d6503d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add Google Drive as an accessible path (Optional if you are running from Jupyter Notebook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# change path to the designated google drive folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_metadata_server\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       ephemeral=ephemeral)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    290\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ac511c5a"},"source":["import praw\n","import pandas as pd\n","import datetime as dt"],"id":"ac511c5a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"73203e92"},"source":["#authorisation\n","my_client_id = 'TiEj498cvvP4ESxTC46fKw'\n","my_client_secret = 'hWJmTE0wHZ8heOGFlghkmo6UDmCqPQ'\n","my_user_agent = 'smt203 project 1'  \n","reddit = praw.Reddit(client_id=my_client_id, client_secret=my_client_secret, user_agent= my_user_agent, check_for_async=False)"],"id":"73203e92","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIKGoRjdlV0O"},"source":["Generate raw data of comments - title of post, user commented, comment body, comment score, comment created date into one dataframe and output as one excel document"],"id":"TIKGoRjdlV0O"},{"cell_type":"code","metadata":{"id":"4d86d122"},"source":["#url = \"https://www.reddit.com/r/singapore/comments/mxxu8t/minister_shanmugams_statement_on_retaining_377a/\"\n","#\"https://www.reddit.com/r/singapore/comments/lzneds/straits_times_transgender_issues_are_real/\"\n","#\"https://www.reddit.com/r/singapore/comments/k18w9w/future_of_lgbt_community_in_singapore/\"\n","#\"https://www.reddit.com/r/TNOmod/comments/lgm6co/on_realism_and_the_lgbt_issue/\"\n","#\"https://www.reddit.com/r/asktransgender/comments/n1p4s4/tiggerwarning_i_may_be_asking_my_transgender/\"\n","#\"https://www.reddit.com/r/singapore/comments/oe66af/christian_group_deeply_concerned_about_singapore/\"\n","#\"https://www.reddit.com/r/SGExams/comments/laf55v/rant_an_update_to_transgender_discrimination_in/\"\n","#\"https://www.reddit.com/r/singapore/comments/nrwsnl/to_all_fellow_lgbt_singaporeans_happy_pride_month/\"\n","#\"https://www.reddit.com/r/singapore/comments/l7qrti/joint_statement_by_asian_ngos_to_condemn_arrest/\"\n","\n","#first url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/mxxu8t/minister_shanmugams_statement_on_retaining_377a/\")\n","submission.comments.replace_more(limit=None)\n","comms_dict = { \"title\":[], \"user\": [], \"body\":[], \"score\":[], \"created\" :[]}\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title) #title of post\n","    comms_dict[\"user\"].append(comment.author) #user commented\n","    comms_dict[\"body\"].append(comment.body) #comment body\n","    comms_dict[\"score\"].append(comment.score) #comment score\n","    comms_dict[\"created\"].append(comment.created) #comment creation date\n"],"id":"4d86d122","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAoTJa7yes8L"},"source":["#2nd url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/lzneds/straits_times_transgender_issues_are_real/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"mAoTJa7yes8L","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97992791"},"source":["#3rd url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/k18w9w/future_of_lgbt_community_in_singapore/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"97992791","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aaQB3VLQfHcT"},"source":["#4th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/TNOmod/comments/lgm6co/on_realism_and_the_lgbt_issue/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"aaQB3VLQfHcT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOcaO_O8fH5K"},"source":["#5th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/asktransgender/comments/n1p4s4/tiggerwarning_i_may_be_asking_my_transgender/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"ZOcaO_O8fH5K","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tfWWNjE7fIKQ"},"source":["#6th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/oe66af/christian_group_deeply_concerned_about_singapore/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"tfWWNjE7fIKQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWhoermqfIZp"},"source":["#7th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/SGExams/comments/laf55v/rant_an_update_to_transgender_discrimination_in/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"tWhoermqfIZp","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A04ePtgafIpn"},"source":["#8th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/nrwsnl/to_all_fellow_lgbt_singaporeans_happy_pride_month/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"A04ePtgafIpn","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNPnbbrgf0DZ"},"source":["#9th url\n","submission = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/l7qrti/joint_statement_by_asian_ngos_to_condemn_arrest/\")\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)"],"id":"GNPnbbrgf0DZ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05b8c131"},"source":["comms_data = pd.DataFrame(comms_dict)"],"id":"05b8c131","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d0beede2"},"source":["comms_data"],"id":"d0beede2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"66a29dcc"},"source":["def get_date(created):\n","    return dt.datetime.fromtimestamp(created)"],"id":"66a29dcc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7b6590d"},"source":["_timestamp = comms_data[\"created\"].apply(get_date)"],"id":"f7b6590d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"287d88c0"},"source":["comms_data = comms_data.assign(timestamp = _timestamp)"],"id":"287d88c0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"965d2ed8"},"source":["comms_data"],"id":"965d2ed8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bc58acd0"},"source":["mypath= \".\"\n","comms_data.to_csv(f'{mypath}/lgbt_reddit_2021.csv', index=False)"],"id":"bc58acd0","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugE1WlRmk4Mm"},"source":["Text analysis\n","1. Data cleaning\n","2. Unigram analysis\n","3. Bigram analysis\n","4. Find Representative words using log odd"],"id":"ugE1WlRmk4Mm"},{"cell_type":"code","metadata":{"id":"Z2_oM2iEmG71"},"source":["#1. data cleaning\n","#step1: Import utility\n","!wget https://raw.githubusercontent.com/anjisun221/css_codes/main/ay21t1/Lab03_text_analysis/smt203util.py\n","from smt203util import *"],"id":"Z2_oM2iEmG71","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"diaKiFS0nnGW"},"source":["#after translating to dictionary\n","mypath= \".\"\n","comms_data = pd.read_csv(f'{mypath}/lgbt_reddit_2021_translated.csv') #csv data to read\n","comms_data\n","# comms_data['title'].value_counts() #how many rows each title has"],"id":"diaKiFS0nnGW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvv7U9h_pGaG"},"source":["#step2: convert text to lowercase, remove punctuations, remove words containing numbers, remove url\n","\n","import re \n","import string\n","\n","def clean_text_round1(text):\n","    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n","    text = str(text)\n","    text = text.lower()\n","    text =  re.sub(r'https?://\\S+', '', text) #remove url\n","    text = re.sub(r'www\\S+', '', text) #remove url\n","    text = re.sub(r\"won\\'t\", \"will not\",text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"let\\'s\", \"let us\",text)\n","    # general\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\",text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\",text)\n","\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '',text)  #remove punctuations. also help to remove '-' eg: input: same-sex -> output: samesex\n","    text = re.sub('\\w*\\d\\w*', '', text) #remove words that contain numbers\n","    text = re.sub(r'[^\\w]', ' ',text) #remove \\n\n","\n","\n","    return text\n","\n","#xing yu: suggest to add extra cleaning round to remove keywords seperately for  lgbt and ceca\n","def clean_text_lgbt(text):\n","    text = str(text)\n","    text = text.lower()\n","    text= re.sub('lgbt','',text) #xing yu: remove lgbt keyword\n","    text = re.sub('377a','',text)#xing yu: remove 377a keyword\n","    text = re.sub('pinkdot','',text) #xing yu: remove pinkdot keyword\n","    text = re.sub('pink dot','',text) #xing yu: remove pink dot keyword in case of variation\n","    return text\n","\n","def clean_text_ceca(text):\n","    text = str(text)\n","    text = text.lower()\n","    text= re.sub('ceca','',text) #xing yu: remove ceca keyword\n","    text = re.sub('foreign talent','',text)#xing yu: remove foreign talent keyword\n","    text = re.sub('foreign talents','',text) #xing yu: remove foreign talents keyword in case of variation\n","    text = re.sub('foreign worker','',text) #xing yu: remove foreign worker keyword \n","    text = re.sub('foreign workers','',text)#xing yu: remove foreign workers keyword in case of variation \n","    return text    "],"id":"xvv7U9h_pGaG","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VeKmiqwRsPT-"},"source":["comms_data['clean_text_1'] = comms_data['translated'].apply(clean_text_round1)\n","# comms_data['clean_text_1'] = comms_data['translated'].apply(clean_text_lgbt)\n","\n","comms_data.head()"],"id":"VeKmiqwRsPT-","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6fiLhpxzCJC"},"source":["#step 3: remove stopwords\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n","def clean_text_round2(text):\n","    return ' '.join([word for word in text.split() if word not in (stop)])\n","\n","comms_data['clean_text_2'] = comms_data['clean_text_1'].apply(clean_text_round2)\n","comms_data['clean_text_3'] = comms_data['clean_text_2'].apply(clean_text_lgbt)\n","comms_data.dropna(axis = 0, subset = ['clean_text_1'], inplace = True) \n","comms_data.dropna(axis = 0, subset = ['clean_text_2'], inplace = True) \n","comms_data.dropna(axis = 0, subset = ['clean_text_3'], inplace = True) \n","\n","comms_data.head()"],"id":"T6fiLhpxzCJC","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ozDATVM_EMO"},"source":["#save cleaned data to csv\n","comms_data.to_csv(f'{mypath}/lgbt_reddit_cleaned_2021.csv', index=False)\n"],"id":"1ozDATVM_EMO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQI-2jiByRow"},"source":["mypath = \".\"\n","comms_data =  pd.read_csv(f'{mypath}/lgbt_reddit_cleaned_2021.csv') #csv data to read\n","comms_data"],"id":"AQI-2jiByRow","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LSqe3CQpFQQs"},"source":["#2. Unigram Analysis\n","\n","import os\n","os.makedirs('word_counts', exist_ok=True)\n","\n","# this function create a dictionary with word counts from dataframe \n","def count_words_from_dataframe(df):\n","    result_dict = {}\n","    # iterate rows of dataframe \n","    for index, row in df.iterrows():\n","        text = str(row['clean_text_3'])\n","        \n","        # this will split a sentence into words \n","        tokens = text.split()\n","        \n","        # iterate each word \n","        for i in range(0, len(tokens)):\n","            token = tokens[i]\n","            try:\n","                result_dict[token] += 1\n","            except KeyError:\n","                result_dict[token] = 1\n","                    \n","    return result_dict\n"],"id":"LSqe3CQpFQQs","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IXeAFHVaMV1L"},"source":["import csv\n","\n","count_type = 'unigram'\n","## this function will return a dictionary of words and frequency\n","result = count_words_from_dataframe(comms_data)\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","with open(f\"./word_counts/lgbt_2021_{count_type}_counts.csv\", 'w') as fp: #csv created\n","      writer = csv.writer(fp, delimiter=',')\n","      writer.writerows(sorted_dic)\n"],"id":"IXeAFHVaMV1L","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuyYJGtDF14D"},"source":["#wordcloud with unigram\n","### Import relevant libraries\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from PIL import Image\n","from os import path\n","import csv\n","\n","### this function will creat wordcloud based on word frequencies and save them into files under plot. \n","def makeImage(termDict, outputfile):\n","    \n","    wc = WordCloud(max_font_size=60, width=1280, height=720, background_color=\"white\")    \n","\n","    ### generate word cloud using frequencies!\n","    wc.generate_from_frequencies(termDict)\n","    wc.to_file(outputfile+\".png\")\n","    wc.to_file(outputfile+\".pdf\")\n","    \n","    ### show the figure\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show() \n","    "],"id":"tuyYJGtDF14D","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-J5HNYEF_gP"},"source":["### Create ```plot``` folder \n","os.makedirs('plot', exist_ok=True)\n","\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/lgbt_2021_unigram_counts.csv') #csv data to read\n","outputfile = f\"./plot/wordcloud_count_unigram_lgbt_2021\"\n","makeImage(fullTermsDict, outputfile)"],"id":"J-J5HNYEF_gP","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wodJcA5POZEU"},"source":["#bigram analysis\n","def count_bigrams_from_dataframe(df):\n","    \n","    result_dict = {}                    \n","    for index, row in df.iterrows():\n","        text = str(row['clean_text_2'])\n","        tokens = text.split()\n","        for itr in range(0, len(tokens) - 1):\n","            token1 = tokens[itr]\n","            token2 = tokens[itr + 1]\n","            token = token1 + \" \" + token2\n","            try:\n","                result_dict[token] += 1\n","            except KeyError:\n","                result_dict[token] = 1    \n","    return result_dict\n","\n","\n","import csv\n","count_type = 'bigram'\n","   \n","result = count_bigrams_from_dataframe(comms_data)\n","\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","    \n","with open(f\"./word_counts/lgbt_2021_{count_type}_counts.csv\", 'w') as fp:\n","    writer = csv.writer(fp, delimiter=',')\n","    writer.writerows(sorted_dic)"],"id":"wodJcA5POZEU","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lnphCv1cGhFX"},"source":["#word cloud with bigram\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/lgbt_2021_bigram_counts.csv') #csv data to read\n","outputfile = f\"./plot/wordcloud_count_bigram_lgbt_2021\"\n","makeImage(fullTermsDict, outputfile)"],"id":"lnphCv1cGhFX","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r1gmiMkmO7Yn"},"source":["Collect data for ceca and foreign talent"],"id":"r1gmiMkmO7Yn"},{"cell_type":"code","metadata":{"id":"OxpNFwNBO5cR"},"source":["#first url for ceca\n","sub_ceca = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/popv5r/a_foreigners_perspective_on_the_ceca_issue/\") #537 comments\n","sub_ceca.comments.replace_more(limit=None)\n","comms_dict_ceca = { \"title\":[], \"user\": [], \"body\":[], \"score\":[], \"created\" :[]}\n","for comment in sub_ceca.comments.list():\n","    comms_dict_ceca[\"title\"].append(sub_ceca.title) #title of post\n","    comms_dict_ceca[\"user\"].append(comment.author) #user commented\n","    comms_dict_ceca[\"body\"].append(comment.body) #comment body\n","    comms_dict_ceca[\"score\"].append(comment.score) #comment score\n","    comms_dict_ceca[\"created\"].append(comment.created) #comment creation date"],"id":"OxpNFwNBO5cR","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F_lUAdwqS2oK"},"source":["#second url for ceca\n","sub_ceca = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/ofbn1w/unpopular_opinion_some_folks_on_this_sub_against/\") #331 comments\n","sub_ceca.comments.replace_more(limit=None)\n","for comment in sub_ceca.comments.list():\n","    comms_dict_ceca[\"title\"].append(sub_ceca.title) #title of post\n","    comms_dict_ceca[\"user\"].append(comment.author) #user commented\n","    comms_dict_ceca[\"body\"].append(comment.body) #comment body\n","    comms_dict_ceca[\"score\"].append(comment.score) #comment score\n","    comms_dict_ceca[\"created\"].append(comment.created) #comment creation date"],"id":"F_lUAdwqS2oK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t7VqbdJlTmm_"},"source":["#first url for foreign talent\n","sub_ceca = reddit.submission(url=\"https://www.reddit.com/r/singapore/comments/pr7x3t/psps_foreign_talent_policy_proposals_will_hurt/\") #137 comments\n","sub_ceca.comments.replace_more(limit=None)\n","for comment in sub_ceca.comments.list():\n","    comms_dict_ceca[\"title\"].append(sub_ceca.title) #title of post\n","    comms_dict_ceca[\"user\"].append(comment.author) #user commented\n","    comms_dict_ceca[\"body\"].append(comment.body) #comment body\n","    comms_dict_ceca[\"score\"].append(comment.score) #comment score\n","    comms_dict_ceca[\"created\"].append(comment.created) #comment creation date"],"id":"t7VqbdJlTmm_","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFlLhw2uUjlV"},"source":["comms_data_ceca = pd.DataFrame(comms_dict_ceca)\n","_timestamp = comms_data_ceca[\"created\"].apply(get_date)\n","comms_data_ceca = comms_data_ceca.assign(timestamp = _timestamp)\n","comms_data_ceca.to_csv(f'{mypath}/ceca_reddit_2021.csv', index=False)\n","\n","comms_data_ceca = pd.read_csv(f'{mypath}/ceca_reddit_2021_translated.csv') #csv data to read\n","comms_data_ceca['title'].value_counts() #how many rows each title has\n","\n"],"id":"yFlLhw2uUjlV","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-c7AcKD_Yz5u"},"source":["comms_data_ceca"],"id":"-c7AcKD_Yz5u","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDz6eCHaZgoc"},"source":["comms_data_ceca['clean_text_1'] = comms_data_ceca['translated'].apply(clean_text_round1)\n","comms_data_ceca.head()"],"id":"cDz6eCHaZgoc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tRq1DcLEZxoF"},"source":["#stopwords removal for ceca\n","comms_data_ceca['clean_text_2'] = comms_data_ceca['clean_text_1'].apply(clean_text_round2)\n","comms_data_ceca['clean_text_3'] = comms_data_ceca['clean_text_2'].apply(clean_text_ceca)\n","comms_data_ceca.dropna(axis = 0, subset = ['clean_text_1'], inplace = True)\n","comms_data_ceca.dropna(axis = 0, subset = ['clean_text_2'], inplace = True)\n","comms_data_ceca.dropna(axis = 0, subset = ['clean_text_3'], inplace = True)\n","comms_data_ceca.head()"],"id":"tRq1DcLEZxoF","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tr0TVSkeaJAt"},"source":["comms_data_ceca.to_csv(f'{mypath}/ceca_reddit_cleaned_2021.csv', index=False) \n"],"id":"Tr0TVSkeaJAt","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_LWHVh87zx7k"},"source":["comms_data_ceca =  pd.read_csv(f'{mypath}/ceca_reddit_cleaned_2021.csv') #csv data to read\n","comms_data_ceca"],"id":"_LWHVh87zx7k","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzM8O3KaalDq"},"source":["#unigram analysis for ceca\n","import csv\n","\n","count_type = 'unigram'\n","## this function will return a dictionary of words and frequency\n","result = count_words_from_dataframe(comms_data_ceca)\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","with open(f\"./word_counts/ceca_2021_{count_type}_counts.csv\", 'w') as fp:\n","      writer = csv.writer(fp, delimiter=',')\n","      writer.writerows(sorted_dic)\n"],"id":"JzM8O3KaalDq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NP20K7LlbJKN"},"source":["#word cloud with ceca unigram analysis\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/ceca_2021_unigram_counts.csv')#csv data to read\n","outputfile = f\"./plot/wordcloud_count_unigram_ceca_2021\"\n","makeImage(fullTermsDict, outputfile)"],"id":"NP20K7LlbJKN","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZvatag1bjR-"},"source":["#bigram\n","import csv\n","count_type = 'bigram'\n","   \n","result = count_bigrams_from_dataframe(comms_data_ceca)\n","\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","    \n","with open(f\"./word_counts/ceca_2021_{count_type}_counts.csv\", 'w') as fp:\n","    writer = csv.writer(fp, delimiter=',')\n","    writer.writerows(sorted_dic)\n","\n","#word cloud with bigram\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/ceca_2021_bigram_counts.csv')\n","outputfile = f\"./plot/wordcloud_count_bigram_ceca_2021\"\n","makeImage(fullTermsDict, outputfile)"],"id":"sZvatag1bjR-","execution_count":null,"outputs":[]}]}