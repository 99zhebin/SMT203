{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reddit Data Collection, Cleaning, Word Counts, Log Odds, Classification.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KzQP3yUSKWaB"},"source":["\n","1. **Extract Reddit Data**\n","\n"]},{"cell_type":"code","metadata":{"id":"Dk1rDC4omi94"},"source":["!pip install praw"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqPRvtZWmkvU"},"source":["!pip install pandas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPXzfT6PpfnS"},"source":["# Add Google Drive as an accessible path (Optional if you are running from Jupyter Notebook)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# change path to the designated google drive folder\n","# otherwise, data will be saved in /content folder which you may have issue locating\n","%cd /content/drive/My Drive/Colab Notebooks/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPApCWnZmp4M"},"source":["#! python3\n","import praw\n","import pandas as pd\n","import datetime as dt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uZdUcHhmqQ8"},"source":["reddit = praw.Reddit(client_id='', \\\n","                     client_secret='_', \\\n","                     user_agent='', \\\n","                     username='', \\\n","                     password='')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDUd6iQMmrgz"},"source":["url = \"\"\n","submission = reddit.submission(url=url)\n","\n","print(submission.title)\n","# Output: the submission's title\n","print(submission.score)\n","# Output: the submission's score\n","print(submission.id)\n","# Output: the submission's ID\n","print(submission.url)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"akoWkEQrs-4N"},"source":["reddit.submissions() < can use submission id (take from above code) or the url link "]},{"cell_type":"code","metadata":{"id":"kDznQCelmyeq"},"source":["import pprint\n","\n","# assume you have a Reddit instance bound to variable `reddit`\n","submission = reddit.submission(url=url)\n","print(submission.title)  # to make it non-lazy\n","pprint.pprint(vars(submission))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EjeW7j0RtJ2m"},"source":["This shows all the comment (all the top level first followed by all the 2nd level, all the 3rd level....)\n"]},{"cell_type":"code","metadata":{"id":"JeNVnKtDm63S"},"source":["submission.comments.replace_more(limit=None)\n","comment_queue = submission.comments[:]  # Seed with top-level\n","while comment_queue:\n","    comment = comment_queue.pop(0)\n","    print(comment.body)\n","    comment_queue.extend(comment.replies)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04xVFwLPJg4X"},"source":["Generate raw data of comments - title of post, user commented, comment body, comment score, comment created date into one dataframe and output as one excel document (Change the URL and run the same code until done)"]},{"cell_type":"code","metadata":{"id":"VXbbddRRQwyB"},"source":["comms_dict = { \"title\": [], \"user\": [], \"body\":[], \"score\":[], \"created\" :[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtF6Yjs1m-2q"},"source":["submission = reddit.submission(url='https://www.reddit.com/r/singapore/comments/hj6tcn/better_jobs_for_singaporeans/')\n","submission.comments.replace_more(limit=None)\n","for comment in submission.comments.list():\n","    comms_dict[\"title\"].append(submission.title)\n","    comms_dict[\"user\"].append(comment.author)\n","    comms_dict[\"body\"].append(comment.body)\n","    comms_dict[\"score\"].append(comment.score)\n","    comms_dict[\"created\"].append(comment.created)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eioMFUV-nCo5"},"source":["comms_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQp4mS5IoN_K"},"source":["comms_data = pd.DataFrame(comms_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KOrBh8wOoRuG"},"source":["comms_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80stS-RqoSfy"},"source":["def get_date(created):\n","    return dt.datetime.fromtimestamp(created)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2xWfypxoYCa"},"source":["_timestamp = comms_data[\"created\"].apply(get_date)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VMKMRJCEovpV"},"source":["comms_data = comms_data.assign(timestamp = _timestamp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4nWnu3oo2sA"},"source":["comms_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-1S5en_pNJ2"},"source":["mypath= \".\"\n","comms_data.to_csv(f'{mypath}/foreigners_2020.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYXif65PPihF"},"source":["2. **Cleaning and EDA**"]},{"cell_type":"markdown","metadata":{"id":"pQUZD4IHPgUV"},"source":["1.   Check CSV file\n","2.   remove empty 'body' columns\n","3.   fill empty username with 'null_username')"]},{"cell_type":"code","metadata":{"id":"5D0rmKahqFg0"},"source":["import pandas as pd\n","import numpy as np\n","mypath= \".\"\n","filename = \"foreigner_2020\"\n","df = pd.read_csv(f'{mypath}/{filename}.csv')\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"atdSxwboP80G"},"source":["df.dropna(axis = 0, subset = ['body'], inplace = True)\n","df.fillna(value='null_username',inplace = True)\n","df.info() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1kZv891YQAXv"},"source":["First round of cleaning\n","\n","1.   Change text to string\n","2.   Lowercase for all\n","3.   Expand contractions\n","4.   Remove punctuations\n","5.   Remove digits in text\n"]},{"cell_type":"code","metadata":{"id":"LN1hgwumP_EY"},"source":["# Apply a first round of text cleaning techniques\n","import re \n","import string\n","\n","#https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n","\n","def clean_text_round1(text):\n","    \n","    '''Make text lowercase, remove punctuation and remove words containing numbers.'''\n","    text = str(text)\n","    text = text.lower()\n","    text = re.sub(r\"won\\'t\", \"will not\",text)\n","    text = re.sub(r\"can\\'t\", \"can not\", text)\n","    text = re.sub(r\"let\\'s\", \"let us\",text)\n","    # general\n","    text = re.sub(r\"n\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'re\", \" are\", text)\n","    text = re.sub(r\"\\'s\", \" is\", text)\n","    text = re.sub(r\"\\'d\", \" would\", text)\n","    text = re.sub(r\"\\'ll\", \" will\",text)\n","    text = re.sub(r\"\\'t\", \" not\", text)\n","    text = re.sub(r\"\\'ve\", \" have\", text)\n","    text = re.sub(r\"\\'m\", \" am\",text)\n","\n","    text = re.sub('[%s]' % re.escape(string.punctuation), '',text)  #xing yu: remove punctuations. also help to remove '-' eg: input: same-sex -> output: samesex\n","    text = re.sub('\\w*\\d\\w*', '', text) #xing yu: remove words that contain numbers\n","    text = re.sub(r'[^\\w]', ' ',text) #remove \\n\n","\n","    text = re.sub(r'http\\S+', '', text) #remove url\n","    text = re.sub(r'www\\S+', '', text) #remove url \n","   \n","    #removes digits in between texts\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrwtRXcgQDIj"},"source":["df['clean_text_1'] = df['body'].apply(clean_text_round1)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJCN-FGqQEj6"},"source":["Second round of cleaning: Remove stopwords"]},{"cell_type":"code","metadata":{"id":"hoUeS45VQGUT"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeQFV2eQQHUM"},"source":["from nltk.corpus import stopwords\n","stop = stopwords.words('english')\n","\n","# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n","def clean_text_round2(text):\n","    return ' '.join([word for word in text.split() if word not in (stop)])\n","#split the words by space and join the words if the word is not in stop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpcSnWuPQIN_"},"source":["df['clean_text_2'] = df['clean_text_1'].apply(clean_text_round2)\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o4Zm5wcsQJTR"},"source":["mypath= \".\"\n","df.to_csv(f'{mypath}/cleaned_{filename}.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwY7m7UYQMtn"},"source":["Open new csv file and drop null clean_text_2"]},{"cell_type":"code","metadata":{"id":"5oI8R6PAQL5a"},"source":["df = pd.read_csv(f'{mypath}/cleaned_{filename}.csv')\n","df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l5wR9iq9QQj3"},"source":["df.dropna(axis = 0, subset = ['clean_text_2'], inplace = True) \n","mypath= \".\"\n","df.to_csv(f'{mypath}/cleaned_{filename}.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnmsJeRlQRxv"},"source":["df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o822uryVRGzn"},"source":["**Text Analysis**"]},{"cell_type":"code","metadata":{"id":"oKE_0amcRJEx"},"source":["### Let's download ```smt203util.py``` Below code should download the file in the same folder where your jupyter notebook is. \n","!wget https://raw.githubusercontent.com/anjisun221/css_codes/main/ay21t1/Lab03_text_analysis/smt203util.py\n","#wget download file from internet (download smt203util.py)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2K7AYAuRKHW"},"source":["### Let's import all function from smt203util! \n","from smt203util import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dlZrqhUERK_x"},"source":["### Import Pandas to analyze the data\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B9eOI1ZmRR2m"},"source":["mypath= \".\"\n","filename = \"\" #use the cleaned one\n","df = pd.read_csv(f'{mypath}/{filename}.csv')\n","df.info()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JXLB9khBRZmC"},"source":["Third round of cleaning to remove keywords separately"]},{"cell_type":"code","metadata":{"id":"IdHP_Io3RYOq"},"source":["\n","#xing yu: suggest to add extra cleaning round to remove keywords seperately for  lgbt and ceca\n","def clean_text_keywords(text):\n","    text = str(text)\n","    text = text.lower()\n","    text= re.sub('lgbt','',text) #xing yu: remove lgbt keyword\n","    text = re.sub('377a','',text)#xing yu: remove 377a keyword\n","    text = re.sub('pinkdot','',text) #xing yu: remove pinkdot keyword\n","    text = re.sub('pink dot','',text) #xing yu: remove pink dot keyword in case of variation\n","\n","    text = str(text)\n","    text = text.lower()\n","    text= re.sub('ceca','',text) #xing yu: remove ceca keyword\n","    text = re.sub('foreign talent','',text)#xing yu: remove foreign talent keyword\n","    text = re.sub('foreign talents','',text) #xing yu: remove foreign talents keyword in case of variation\n","    text = re.sub('foreign worker','',text) #xing yu: remove foreign worker keyword \n","    text = re.sub('foreign workers','',text)#xing yu: remove foreign workers keyword in case of variation \n","\n","    text = re.sub(r'http\\S+', '', text) #remove url\n","    text = re.sub(r'www\\S+', '', text) #remove url \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6gG3YJxdRe0B"},"source":["df['clean_text_3'] = df['clean_text_2'].apply(clean_text_keywords)\n","df.head()\n","df.to_csv(f'{mypath}/{filename}.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kd-cmJR1Rg25"},"source":["Unigram analysis - Counting words (and save it to a file)"]},{"cell_type":"code","metadata":{"id":"iZfvEZP5RhOr"},"source":["## Create ```word_counts``` folder \n","import os\n","os.makedirs('word_counts', exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nWoN93vRkiV"},"source":["# this function create a dictionary with word counts from dataframe \n","def count_words_from_dataframe(df):\n","    result_dict = {}\n","    # iterate rows of dataframe \n","    for index, row in df.iterrows():\n","        text = str(row['CleanText3'])\n","        #return the text for each row\n","        \n","        # this will split a sentence into words \n","        tokens = text.split()\n","        #split the sentence by space >> list\n","        \n","        # iterate each word and count the number of words it appears in each of the speech for the list of speech\n","        for i in range(0, len(tokens)):\n","            token = tokens[i]\n","            try:\n","                result_dict[token] += 1\n","            except KeyError:\n","                result_dict[token] = 1\n","                    \n","    return result_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtHl09ISRl3I"},"source":["import csv\n","\n","count_type = 'unigram'\n","## this function will return a dictionary of words and frequency\n","result = count_words_from_dataframe(df)\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","with open(f\"./word_counts/{filename}_{count_type}_counts.csv\", 'w') as fp:\n","      writer = csv.writer(fp, delimiter=',')\n","      writer.writerows(sorted_dic)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcXQn8P5Rj8u"},"source":["Draw Wordcloud using unigrams"]},{"cell_type":"code","metadata":{"id":"KmflEKb3RpkP"},"source":["## This it OPTIONAL if you are running the current notebook using Google Colab\n","!conda install --yes -c conda-forge wordcloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4qenx6zRqe8"},"source":["### Import relevant libraries\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from PIL import Image\n","from os import path\n","import csv\n","\n","### this function will creat wordcloud based on word frequencies and save them into files under plot. \n","def makeImage(termDict, outputfile):\n","    \n","    wc = WordCloud(max_font_size=60, width=1280, height=720, background_color=\"white\")    \n","\n","    ### generate word cloud using frequencies!\n","    wc.generate_from_frequencies(termDict)\n","    wc.to_file(outputfile+\".png\")\n","    wc.to_file(outputfile+\".pdf\")\n","    \n","    ### show the figure\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show() \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSuB9gRlRrkg"},"source":["### Create ```plot``` folder \n","os.makedirs('plot', exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a14PTC3uRsZe"},"source":["\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{filename}_unigram_counts.csv')\n","outputfile = f\"./plot/wordcloud_count_unigram_{filename}\"\n","makeImage(fullTermsDict, outputfile)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G87rafCSRty4"},"source":["Count Bigram"]},{"cell_type":"code","metadata":{"id":"Z_CUwdDcRvGJ"},"source":["def count_bigrams_from_dataframe(df):\n","    \n","    result_dict = {}                    \n","    for index, row in df.iterrows():\n","        text = row['CleanText2']\n","        tokens = str(text).split()\n","        for i in range(0, len(tokens)-1):\n","          token = tokens[i] + \" \" + tokens[i+1]\n","          try:\n","              result_dict[token] += 1\n","          except KeyError:\n","              result_dict[token] = 1\n","\n","        ## write your code \n","        \n","    return result_dict\n","\n","\n","import csv\n","count_type = 'bigram'\n","\n","result = count_bigrams_from_dataframe(df)\n","    \n","\n","## soring the words based on their frequency\n","sorted_dic = ((k, result[k]) for k in sorted(result, key=result.get, reverse=True))\n","\n","## write the dictionary in a file\n","with open(f\"./word_counts/{filename}_{count_type}_counts.csv\", 'w') as fp:\n","    writer = csv.writer(fp, delimiter=',')\n","    writer.writerows(sorted_dic)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGFXMua1RwoN"},"source":["#word cloud with bigram\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/word_counts/{filename}_bigram_counts.csv')\n","outputfile = f\"./plot/wordcloud_count_bigram_{filename}\"\n","makeImage(fullTermsDict, outputfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gximZ6nlR253"},"source":["Let's load word frequency file and create dictionary of word counts for the two platforms (NEED DATA FROM HARDWAREZONE AND REDDIT)"]},{"cell_type":"code","metadata":{"id":"CMuQ-o3sR1cw"},"source":["#counts_i_name ='lgbt_reddit_cleaned_2021'\n","#counts_i_name ='ceca_reddit_cleaned_2021'\n","#counts_i_name ='REDDIT2020LGBTFINAL'\n","counts_i_name ='REDDIT2020CECAFINAL'\n","\n","counts_i = read_word_count_file(f\"{mypath}/word_counts/{counts_i_name}_unigram_counts.csv\")\n","\n","### we filter out words that doesnt exist in our background corpus\n","counts_i_dict = {k: v for k, v in counts_i.items() if k in global_counts}\n","#check whether that word is in global_count"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9HdxJ06R59O"},"source":["#counts_j_name = 'HWZ2021LGBT'\n","#counts_j_name = 'HWZ2021CECA'\n","#counts_j_name = 'HWZEDMW2020LGBTFINAL'\n","counts_j_name = 'HWZEDMW2020CECAFINAL'\n","\n","counts_j = read_word_count_file(f\"{mypath}/word_counts/{counts_j_name}_unigram_counts.csv\")\n","\n","### we filter out words that exist in our background corpus\n","counts_j_dict = {k: v for k, v in counts_j.items() if k in global_counts}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CW7YX6qR62C"},"source":["### this function will return log-odds values. \n","top_words_df = calculate_log_odds_idp(global_counts, counts_i_name, counts_i_dict, counts_j_name, counts_j_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsbKIqbLSBqX"},"source":["Interpretation lad z-score (log_odds_z_score)"]},{"cell_type":"code","metadata":{"id":"uH6NS6oOSDG5"},"source":["\n","#top_words_df[top_words_df[counts_i_name] >= 2].iloc[::-1].head(20)\n","\n","top_words_df.sort_values(by = [counts_i_name], ascending=False).head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xlJ3XepqSEO5"},"source":["\n","#top_words_df[top_words_df[counts_j_name] >= 2].head(20)\n","top_words_df.sort_values(by = [counts_j_name], ascending=False).head(20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eSxXUVPLSFNB"},"source":["### Below function will simply help you to print the above table into file, so that we can use it for drawing word cloud. \n","### The output of this function is a csv file where each row contains (word, log_odds_z_score) for the two corpora. \n","### threshold_i and threshold_j are the threshold of word frequency. \n","### num_i and num_j are the number of representative words it will write in file. \n","find_discriminative_words(top_words_df, threshold_i=10, threshold_j=10, num_i=20, num_j=20, mypath='.')\n","#larger dataset > set higher threshold\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kogHpfJUSIUi"},"source":["Draw WordClouds based on log odds values"]},{"cell_type":"markdown","metadata":{"id":"cR8rREwvSanr"},"source":["Draw a WordCloud for Reddit"]},{"cell_type":"code","metadata":{"id":"hL-irWIqSI6b"},"source":["\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from PIL import Image\n","from os import path\n","import csv\n"," \n","### this function will creat wordcloud based on word frequencies and save them into files under plot. \n","def makeImage(termDict, outputfile):\n","\n","    wc = WordCloud(width=500, height=300, background_color=\"black\",colormap=\"Reds\")    \n"," \n","    ### generate word cloud using frequencies!\n","    wc.generate_from_frequencies(termDict)\n","    wc.to_file(outputfile+\".png\")\n","    wc.to_file(outputfile+\".pdf\")\n","    \n","    ### show the figure\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show() "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VmMIU5a1SKSV"},"source":["### Create ```plot``` folder \n","os.makedirs('plot', exist_ok=True)\n"," \n","filename = counts_i_name\n","\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/{filename}_zscore.csv')\n","outputfile = f\"./plot/wordcloud_count_zscore_{filename}\"\n","makeImage(fullTermsDict, outputfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8G1xPOhSXWy"},"source":["Draw a WordCloud for hardwarezone"]},{"cell_type":"code","metadata":{"id":"A4CJdrXdSLgd"},"source":["\n","\n","from wordcloud import WordCloud, STOPWORDS\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import re\n","from PIL import Image\n","from os import path\n","import csv\n"," \n","### this function will creat wordcloud based on word frequencies and save them into files under plot. \n","def makeImage(termDict, outputfile):\n","\n","    wc = WordCloud(width=500, height=300, background_color=\"black\",colormap=\"Greens\")    \n"," \n","    ### generate word cloud using frequencies!\n","    wc.generate_from_frequencies(termDict)\n","    wc.to_file(outputfile+\".png\")\n","    wc.to_file(outputfile+\".pdf\")\n","    \n","    ### show the figure\n","    plt.imshow(wc, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.show() \n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owsqeC8YSMxL"},"source":["### Create ```plot``` folder \n","os.makedirs('plot', exist_ok=True)\n"," \n","filename = counts_j_name\n","\n","### We're using one of the utility function! \"read_word_count_file\"\n","fullTermsDict = read_word_count_file(f'{mypath}/{filename}_zscore.csv')\n","outputfile = f\"./plot/wordcloud_count_zscore_{filename}\"\n","makeImage(fullTermsDict, outputfile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AdDOXu4hTeQG"},"source":["3. **Classification**"]},{"cell_type":"code","metadata":{"id":"3J6AOnVRTki1"},"source":["import numpy as np\n","import pandas as pd\n","\n","# Packages for train/test dataset split\n","from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IkPNU2KT4J4"},"source":["!pip install datasets transformers[sentencepiece]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ksg3mrLeT6aG"},"source":["import torch\n","device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jN_oTxsuT7vc"},"source":["from transformers import pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tf69o8pkTtnr"},"source":["mypath = '.'\n","df = pd.read_csv(f'{mypath}/.csv',encoding= 'unicode_escape')\n","df.info()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCa6cvdITzIY"},"source":["df['sentiment'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-AV92jPbT9Ct"},"source":["sentences = list(df['clean_text_1'].iloc[0:200].values)\n","y_str = list(df['sentiment'].iloc[0:200].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGTGLw8zUIJk"},"source":["len(y_str)\n","len(sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hkqJnJHDUI58"},"source":["pred_sentences = list(df['clean_text_1'].values)\n","len(pred_sentences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"goBg05whUKC9"},"source":["y = []\n","for i in range(len(y_str)):\n","    if y_str[i] == \"positive\":\n","        y.append(0)\n","      \n","    elif y_str[i] == \"negative\":\n","        y.append(1)\n","  \n","    elif y_str[i] == \"neutral\":\n","        y.append(2) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wOf0qvA1YVWw"},"source":["Training dataset"]},{"cell_type":"code","metadata":{"id":"9ZCoNlouULkb"},"source":["sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=999)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35vzPjb5UMZb"},"source":["len(sentences_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"841ujjMWUQwq"},"source":["sentences_train, sentences_val, y_train, y_val = train_test_split(sentences_train, y_train, test_size=.2, random_state=999)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7KNCf1kUTV4"},"source":["from transformers import DistilBertTokenizerFast\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vj7YkEJyUUYA"},"source":["train_encodings = tokenizer(sentences_train, truncation=True, padding=True)\n","val_encodings = tokenizer(sentences_val, truncation=True, padding=True)\n","test_encodings = tokenizer(sentences_test, truncation=True, padding=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7yivXpUUWC6"},"source":["import torch\n","\n","class myDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PnvTToeUXHo"},"source":["train_dataset = myDataset(train_encodings, y_train)\n","val_dataset = myDataset(val_encodings, y_val)\n","test_dataset = myDataset(test_encodings, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUPe5Nf7UYQ6"},"source":["from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=40,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=64,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n",")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e055iMTsUZex"},"source":["# if it's not a binary classification, num_labels should be given! \n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")\n","\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZDJjfqOUbVm"},"source":["from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vtx3lBgqUcZn"},"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g11yBe_4UdwP"},"source":["trainer.save_model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZ1T6fllUeyg"},"source":["new_model = DistilBertForSequenceClassification.from_pretrained(\"./results\", num_labels=3)\n","\n","new_trainer = Trainer(\n","    model=new_model,                         # the instantiated 🤗 Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset             # evaluation dataset\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MD8VodhVYgT-"},"source":["Creating prediction column"]},{"cell_type":"code","metadata":{"id":"Cw0xQHBPUgmG"},"source":["# create dataset for prediction\n","new_encodings = tokenizer(pred_sentences, truncation=True, padding=True)\n","# create dummy labels with the number of sentences to predict. \n","y_new = np.full(len(pred_sentences), 1)\n","new_dataset = myDataset(new_encodings, y_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-OLbPzHUieV"},"source":["new_predictions = new_trainer.predict(new_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zc7i7kDSUkGD"},"source":["new_preds = np.argmax(new_predictions.predictions, axis=-1)\n","sump = 0\n","sumn = 0\n","sumneg = 0\n","for i in new_preds:\n","  if i == 0:\n","    sump += 1\n","  elif i == 1:\n","    sumneg += 1\n","  else:\n","    sumn += 1\n","print(f'There are {sump} positive comments, {sumneg} negative comments and {sumn} neutral comments')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hZTx8J42Ulua"},"source":["df['Prediction'] = new_preds.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7LoI7nfaUnSU"},"source":["def add_predictions(pred):\n","  if pred == 0:\n","    pred = 'pro-lgbt'\n","  elif pred == 1:\n","    pred = 'anti-lgbt'\n","  else:\n","    pred = 'neutral'\n","  return pred "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QK_Dt8AaUoBb"},"source":["df['Prediction'] = df['Prediction'].apply(add_predictions)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1_S9fLOUrJn"},"source":["df['Prediction'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbgVYv8DUr_Z"},"source":["df.to_csv(f'{mypath}/.csv', index=False)"],"execution_count":null,"outputs":[]}]}